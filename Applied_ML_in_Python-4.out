\BOOKMARK [1][-]{section.1}{Introduction to Supervised Machine Learning}{}% 1
\BOOKMARK [1][-]{section.2}{Overfitting and Underfitting}{}% 2
\BOOKMARK [1][-]{section.3}{Supervised Learning: Datasets}{}% 3
\BOOKMARK [1][-]{section.4}{K-Nearest Neighbors Classification}{}% 4
\BOOKMARK [1][-]{section.5}{K-Nearest Neighbors: Classification and Regression}{}% 5
\BOOKMARK [1][-]{section.6}{Linear Regression: Least-Squares}{}% 6
\BOOKMARK [1][-]{section.7}{Linear Regression: Ridge, Lasso, and Polynomial Regression}{}% 7
\BOOKMARK [1][-]{section.8}{Logistic Regression}{}% 8
\BOOKMARK [1][-]{section.9}{Linear Classifiers: Support Vector Machines}{}% 9
\BOOKMARK [1][-]{section.10}{Multi-Class Classification}{}% 10
\BOOKMARK [1][-]{section.11}{Kernelized Support Vector Machines}{}% 11
\BOOKMARK [1][-]{section.12}{Cross-Validation}{}% 12
\BOOKMARK [1][-]{section.13}{Decision Trees}{}% 13
\BOOKMARK [1][-]{section.14}{Model Evaluation and Selection}{}% 14
\BOOKMARK [2][-]{subsection.14.1}{Evaluation metrics}{section.14}% 15
\BOOKMARK [2][-]{subsection.14.2}{Evaluation metrics for an imbalanced class}{section.14}% 16
\BOOKMARK [2][-]{subsection.14.3}{A null accuracy baseline}{section.14}% 17
\BOOKMARK [3][-]{subsubsection.14.3.1}{With Dummy Classifier}{subsection.14.3}% 18
\BOOKMARK [3][-]{subsubsection.14.3.2}{Other Types of Dummy Classifiers}{subsection.14.3}% 19
\BOOKMARK [2][-]{subsection.14.4}{Binary Classification Outcomes}{section.14}% 20
\BOOKMARK [1][-]{section.15}{Confusion Matrices and Basic Evaluation Metrics}{}% 21
\BOOKMARK [2][-]{subsection.15.1}{Accuracy}{section.15}% 22
\BOOKMARK [2][-]{subsection.15.2}{Classification Error}{section.15}% 23
\BOOKMARK [2][-]{subsection.15.3}{Recall / True Positive Rate / Sensitivity}{section.15}% 24
\BOOKMARK [2][-]{subsection.15.4}{Precision}{section.15}% 25
\BOOKMARK [2][-]{subsection.15.5}{False Positive Rate / Specificity}{section.15}% 26
\BOOKMARK [2][-]{subsection.15.6}{Metrics in Scikit-learn}{section.15}% 27
\BOOKMARK [1][-]{section.16}{Classifier Decision Functions}{}% 28
\BOOKMARK [2][-]{subsection.16.1}{Decision function method}{section.16}% 29
\BOOKMARK [2][-]{subsection.16.2}{Predict proba function}{section.16}% 30
\BOOKMARK [2][-]{subsection.16.3}{Decision threshold}{section.16}% 31
\BOOKMARK [1][-]{section.17}{Precision-Recall and ROC curves}{}% 32
\BOOKMARK [2][-]{subsection.17.1}{Precision-Recall Curves}{section.17}% 33
\BOOKMARK [2][-]{subsection.17.2}{Receiver Operating Characteristic \(ROC\) curves}{section.17}% 34
\BOOKMARK [2][-]{subsection.17.3}{Area Under the ROC Curve \(AUC\)}{section.17}% 35
\BOOKMARK [1][-]{section.18}{Multi-Class Evaluation}{}% 36
\BOOKMARK [1][-]{section.19}{Regression Evaluation}{}% 37
\BOOKMARK [2][-]{subsection.19.1}{R2 score}{section.19}% 38
\BOOKMARK [2][-]{subsection.19.2}{Alternative metrics}{section.19}% 39
\BOOKMARK [3][-]{subsubsection.19.2.1}{Mean absolute error}{subsection.19.2}% 40
\BOOKMARK [3][-]{subsubsection.19.2.2}{Mean squared error}{subsection.19.2}% 41
\BOOKMARK [3][-]{subsubsection.19.2.3}{Median absolute error}{subsection.19.2}% 42
\BOOKMARK [2][-]{subsection.19.3}{Dummy Regressors}{section.19}% 43
\BOOKMARK [1][-]{section.20}{Model Selection: Optimizing Classifiers for Different Evaluation Metrics}{}% 44
\BOOKMARK [2][-]{subsection.20.1}{Cross Validation}{section.20}% 45
\BOOKMARK [2][-]{subsection.20.2}{Grid Search}{section.20}% 46
\BOOKMARK [1][-]{section.21}{Conclusion}{}% 47
\BOOKMARK [1][-]{section.22}{Naive Bayes Classifiers}{}% 48
\BOOKMARK [2][-]{subsection.22.1}{Bernoulli Naive Bayes model}{section.22}% 49
\BOOKMARK [2][-]{subsection.22.2}{Multinomial Naive Bayes model}{section.22}% 50
\BOOKMARK [2][-]{subsection.22.3}{Gaussian Naive Bayes model}{section.22}% 51
\BOOKMARK [1][-]{section.23}{Random Forests}{}% 52
\BOOKMARK [2][-]{subsection.23.1}{Random Forests Model Creation}{section.23}% 53
\BOOKMARK [2][-]{subsection.23.2}{Prediction}{section.23}% 54
\BOOKMARK [2][-]{subsection.23.3}{Implementation}{section.23}% 55
\BOOKMARK [2][-]{subsection.23.4}{Pros and Cons}{section.23}% 56
\BOOKMARK [2][-]{subsection.23.5}{Key Parameters}{section.23}% 57
\BOOKMARK [1][-]{section.24}{Gradient Boosted Decision Trees}{}% 58
\BOOKMARK [2][-]{subsection.24.1}{Pros and Cons}{section.24}% 59
\BOOKMARK [2][-]{subsection.24.2}{Key Parameters}{section.24}% 60
\BOOKMARK [1][-]{section.25}{Neural-Networks}{}% 61
\BOOKMARK [2][-]{subsection.25.1}{Implementation}{section.25}% 62
\BOOKMARK [2][-]{subsection.25.2}{Multi-layer Perceptron}{section.25}% 63
\BOOKMARK [2][-]{subsection.25.3}{Regularization}{section.25}% 64
\BOOKMARK [2][-]{subsection.25.4}{Features Normalization}{section.25}% 65
\BOOKMARK [2][-]{subsection.25.5}{Regression}{section.25}% 66
\BOOKMARK [2][-]{subsection.25.6}{Pros and Cons}{section.25}% 67
\BOOKMARK [2][-]{subsection.25.7}{Key Parameters}{section.25}% 68
\BOOKMARK [1][-]{section.26}{Deep Learning}{}% 69
\BOOKMARK [2][-]{subsection.26.1}{Pros and Cons}{section.26}% 70
\BOOKMARK [1][-]{section.27}{Data Leakage}{}% 71
\BOOKMARK [2][-]{subsection.27.1}{Detecting Data Leakage}{section.27}% 72
\BOOKMARK [2][-]{subsection.27.2}{Minimizing Data Leakage}{section.27}% 73
